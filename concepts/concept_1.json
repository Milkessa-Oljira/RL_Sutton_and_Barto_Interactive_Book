[
  {
    "type": "text",
    "title": "What is Temporal Difference Learning?",
    "content": "Temporal Difference (TD) learning is a key method in Reinforcement Learning that merges the sample-based learning of Monte Carlo methods with the bootstrapping of dynamic programming. Unlike Monte Carlo, which waits for an episode to complete before updating value estimates, TD learning updates estimates incrementally after each step using the TD error—the difference between the predicted value and a refined estimate based on the next state. This allows for efficient, real-time learning in sequential decision-making problems.",
    "background_color": "#fffaf0",
    "border_color": "#e0e0e0",
    "border_radius": "12px",
    "padding": "20px",
    "title_font": "Georgia",
    "title_size": 24,
    "title_color": "#333333",
    "content_font": "Times New Roman",
    "content_size": 16,
    "content_color": "#444444"
  },
  {
    "type": "text",
    "title": "How TD Learning Compares to Other RL Methods",
    "content": "TD learning distinguishes itself from Monte Carlo methods by not requiring complete episodes. It leverages bootstrapping to update value estimates incrementally, making it both faster and more suitable for online learning. In contrast to dynamic programming, TD learning is model-free, meaning it doesn't need a complete model of the environment. However, this reliance on bootstrapping introduces bias even as it reduces variance.",
    "background_color": "#f0f8ff",
    "border_color": "#dcdcdc",
    "border_radius": "12px",
    "padding": "20px",
    "title_font": "Georgia",
    "title_size": 24,
    "title_color": "#333333",
    "content_font": "Times New Roman",
    "content_size": 16,
    "content_color": "#444444"
  },
  {
    "type": "math",
    "title": "TD(0) Update Rule",
    "content": "V(S_t) \\leftarrow V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]"
  },
  {
    "type": "pseudocode",
    "title": "TD(0) Pseudocode",
    "content": "Initialize V(s) = 0 for all states s\nFor each episode:\n    S ← initial state\n    While not terminal:\n        A ← policy(S)\n        Take action A, observe R, S'\n        V(S) ← V(S) + α [R + γ V(S') - V(S)]\n        S ← S'"
  },
  {
    "type": "code",
    "language": "python",
    "title": "TD(0) Implementation",
    "content": "from collections import defaultdict\nimport numpy as np\n\ndef td_zero(env, num_episodes, alpha=0.1, gamma=0.99):\n    V = defaultdict(float)\n    for _ in range(num_episodes):\n        state = env.reset()\n        done = False\n        while not done:\n            action = env.action_space.sample()  # Placeholder policy\n            next_state, reward, done, _ = env.step(action)\n            V[state] += alpha * (reward + gamma * V[next_state] - V[state])\n            state = next_state\n    return V"
  },
  {
    "type": "text",
    "title": "Understanding the TD Error",
    "content": "Central to TD learning is the TD error, δ = R + γV(S') - V(S), which measures the gap between the current value estimate and a better estimate that includes the reward and the next state's value. This error is used to incrementally update V(S) and drives the learning process.",
    "background_color": "#f5f5dc",
    "border_color": "#c0c0c0",
    "border_radius": "12px",
    "padding": "20px",
    "title_font": "Georgia",
    "title_size": 24,
    "title_color": "#333333",
    "content_font": "Times New Roman",
    "content_size": 16,
    "content_color": "#444444"
  },
  {
    "type": "text",
    "title": "Applications of TD Learning",
    "content": "TD learning is applied in diverse fields—from TD-Gammon in gaming and robotics for navigation to finance for optimizing trading strategies. Its ability to learn from partial episodes makes it ideal for real-time systems like autonomous driving or adaptive control.",
    "background_color": "#f0fff0",
    "border_color": "#dcdcdc",
    "border_radius": "12px",
    "padding": "20px",
    "title_font": "Georgia",
    "title_size": 24,
    "title_color": "#333333",
    "content_font": "Times New Roman",
    "content_size": 16,
    "content_color": "#444444"
  },
  {
    "type": "flow_chart",
    "title": "TD Learning Process Flow",
    "flowchart_data": {
      "nodes": [
        "Start",
        "Initialize V(s)",
        "Start Episode",
        "Choose Action",
        "Take Action",
        "Observe R, S'",
        "Compute TD Error",
        "Update V(s)",
        "Done?",
        "End Episode",
        "End"
      ],
      "edges": [
        ["Start", "Initialize V(s)"],
        ["Initialize V(s)", "Start Episode"],
        ["Start Episode", "Choose Action"],
        ["Choose Action", "Take Action"],
        ["Take Action", "Observe R, S'"],
        ["Observe R, S'", "Compute TD Error"],
        ["Compute TD Error", "Update V(s)"],
        ["Update V(s)", "Done?"],
        ["Done?", "Choose Action", "No"],
        ["Done?", "End Episode", "Yes"],
        ["End Episode", "Start Episode"],
        ["End Episode", "End"]
      ]
    }
  },
  {
    "type": "text",
    "title": "Strengths and Weaknesses of TD Learning",
    "content": "Strengths:\n- Learns from incomplete episodes (online learning)\n- More sample-efficient than Monte Carlo methods\n- Model-free and adaptable to various environments\n\nWeaknesses:\n- Introduces bias due to bootstrapping\n- Sensitive to learning rate (α) and discount factor (γ)\n- May struggle with convergence in highly non-stationary or noisy environments",
    "background_color": "#fff0f5",
    "border_color": "#dcdcdc",
    "border_radius": "12px",
    "padding": "20px",
    "title_font": "Georgia",
    "title_size": 24,
    "title_color": "#333333",
    "content_font": "Times New Roman",
    "content_size": 16,
    "content_color": "#444444"
  },
  {
    "type": "math",
    "title": "SARSA Update Rule",
    "content": "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]"
  },
  {
    "type": "math",
    "title": "Q-Learning Update Rule",
    "content": "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)]"
  },
  {
    "type": "code",
    "language": "python",
    "title": "Q-Learning Implementation",
    "content": "def q_learning(env, num_episodes, alpha=0.1, gamma=0.99, epsilon=0.1):\n    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n    for _ in range(num_episodes):\n        state = env.reset()\n        done = False\n        while not done:\n            if np.random.rand() < epsilon:\n                action = env.action_space.sample()\n            else:\n                action = np.argmax(Q[state])\n            next_state, reward, done, _ = env.step(action)\n            best_next_action = np.argmax(Q[next_state])\n            td_target = reward + gamma * Q[next_state][best_next_action]\n            Q[state][action] += alpha * (td_target - Q[state][action])\n            state = next_state\n    return Q"
  },
  {
    "type": "interactive_code",
    "title": "Interactive Q-Learning in Frozen Lake",
    "content": "import gymnasium as gym\nfrom collections import defaultdict\nimport numpy as np\nenv = gym.make('FrozenLake-v1')\nQ = defaultdict(lambda: np.zeros(env.action_space.n))\nfor _ in range(1000):\n    state = env.reset()\n    if isinstance(state, tuple): state = state[0]  # Handle new gymnasium API\n    done = False\n    while not done:\n        action = np.argmax(Q[state]) if np.random.rand() > 0.1 else env.action_space.sample()\n        next_state, reward, done, _, _ = env.step(action)\n        if isinstance(next_state, tuple): next_state = next_state[0]  # Handle new API\n        Q[state][action] += 0.1 * (reward + 0.99 * np.max(Q[next_state]) - Q[state][action])\n        state = next_state\nstate = env.reset()\nif isinstance(state, tuple): state = state[0]\ndone = False\nwhile not done:\n    action = np.argmax(Q[state])\n    next_state, reward, done, _, _ = env.step(action)\n    if isinstance(next_state, tuple): state = next_state[0]\n    else: state = next_state\n    env.render()"
  },
  {
    "type": "table",
    "title": "Comparison of TD Learning Algorithms",
    "headers": ["Algorithm", "Update Rule", "Policy Type", "Key Feature"],
    "rows": [
      ["TD(0)", "V(S) ← V(S) + α[R + γV(S') - V(S)]", "On-policy", "Simple value estimation"],
      ["SARSA", "Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]", "On-policy", "Safe exploration via policy consistency"],
      ["Q-Learning", "Q(S,A) ← Q(S,A) + α[R + γmax_a Q(S',a) - Q(S,A)]", "Off-policy", "Learning optimal policy with exploration"]
    ]
  },
  {
    "type": "pseudocode",
    "title": "SARSA Pseudocode",
    "content": "Initialize Q(s,a) = 0 for all s, a\nFor each episode:\n    S ← initial state\n    A ← ε-greedy(Q, S)\n    While not terminal:\n        Take action A, observe R, S'\n        A' ← ε-greedy(Q, S')\n        Q(S,A) ← Q(S,A) + α [R + γ Q(S',A') - Q(S,A)]\n        S ← S'\n        A ← A'"
  },
  {
    "type": "pseudocode",
    "title": "Q-Learning Pseudocode",
    "content": "Initialize Q(s,a) = 0 for all s, a\nFor each episode:\n    S ← initial state\n    While not terminal:\n        A ← ε-greedy(Q, S)\n        Take action A, observe R, S'\n        Q(S,A) ← Q(S,A) + α [R + γ max_a Q(S',a) - Q(S,A)]\n        S ← S'"
  },
  {
    "type": "static_chart",
    "title": "TD(0) Value Convergence",
    "chart_type": "line",
    "data": {
      "x": [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
      "y": [0.0, 0.05, 0.12, 0.25, 0.38, 0.49, 0.58, 0.65, 0.71, 0.75, 0.78]
    },
    "x_label": "Episodes",
    "y_label": "Value of Initial State",
    "description": "This chart illustrates the convergence of the value estimate for the initial state over 100 episodes using TD(0) with α=0.1 and γ=0.99."
  }
]
